---
title: "ST 558 Final Project EDA Document"
format: html
author: "Taylor Cesarski"
---

# Introduction

# logLoss
log loss is calculated using the following formula:
`-\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]`




# Read in data
```{r}
library(tidyverse)
diabetes_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

diabetes_data$Diabetes_binary <- factor(diabetes_data$Diabetes_binary, levels = c(0, 1), labels = c("no_diabetes", "diabetes"))

diabetes_data$HighBP <- factor(diabetes_data$HighBP, levels = c(0, 1), labels = c("no_hbp", "hbp"))

diabetes_data$HighChol <- factor(diabetes_data$HighChol, levels = c(0, 1), labels = c("no_high_chol", "high_chol"))

diabetes_data$CholCheck <- factor(diabetes_data$CholCheck, levels = c(0, 1), labels = c("no_chol_check", "chol_check"))

diabetes_data$Smoker <- factor(diabetes_data$Smoker, levels = c(0, 1), labels = c("non_smoker", "smoker"))

diabetes_data$Stroke <- factor(diabetes_data$Stroke, levels = c(0, 1), labels = c("no_stroke", "stroke"))

diabetes_data$HeartDiseaseorAttack <- factor(diabetes_data$HeartDiseaseorAttack, levels = c(0, 1), labels = c("no_hd_or_attack", "hd_or_attack"))

diabetes_data$PhysActivity <- factor(diabetes_data$PhysActivity, levels = c(0, 1), labels = c("no_activity", "activity"))

diabetes_data$Fruits <- factor(diabetes_data$Fruits, levels = c(0, 1), labels = c("no_fruits", "fruits"))

diabetes_data$Veggies <- factor(diabetes_data$Veggies, levels = c(0, 1), labels = c("no_veggies", "veggies"))

diabetes_data$HvyAlcoholConsump <- factor(diabetes_data$HvyAlcoholConsump, levels = c(0, 1), labels = c("no_heavy_alc", "heavy_alc"))

diabetes_data$AnyHealthcare <- factor(diabetes_data$AnyHealthcare, levels = c(0, 1), labels = c("no_healthcare", "healthcare"))

diabetes_data$NoDocbcCost <- factor(diabetes_data$NoDocbcCost, levels = c(0, 1), labels = c("no_cost_issue", "cost_issue"))

diabetes_data$GenHlth <- factor(diabetes_data$GenHlth, levels = c(1, 2, 3, 4, 5), labels = c("excellent", "very_good", "good", "fair", "poor"))

diabetes_data$DiffWalk <- factor(diabetes_data$DiffWalk, levels = c(0, 1), labels = c("no_difficulty", "difficulty"))

diabetes_data$Sex <- factor(diabetes_data$Sex, levels = c(0, 1), labels = c("female", "male"))

diabetes_data$Age <- factor(diabetes_data$Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), labels = c("18_24", "25_29", "30_34", "35_39", "40_44", "45_49", "50_54", "55_59", "60_64", "65_69", "70_74", "75_79", "80_or_more"))

diabetes_data$Education <- factor(diabetes_data$Education, levels = c(1, 2, 3, 4, 5, 6), labels = c("none", "elementary", "some_hs", "high_school", "some_college", "college_grad"))

diabetes_data$Income <- factor(diabetes_data$Income, levels = c(1, 2, 3, 4, 5, 6, 7, 8), labels = c("less_10k", "10k_15k", "15k_20k", "20k_25k", "25k_35k", "35k_50k", "50k_75k", "more_75k"))
```



# Split the Data
First we are going to split the data into a training and test set. We do this so that we can evaluate the model on data that wasn't used to fit the model. This helps us not to overfit the model and make sure that the model can generalize to data it hasn't yet seen.
```{r}
library(caret)
#Set the seed at 100 for reproducibility.
set.seed(100)

#Use the createDataPartition from the caret package. Get 70% of the rows.
trainIndex <- createDataPartition(diabetes_data$Diabetes_binary, p =0.7, list = FALSE)
#Assign those 70% to the training dataset.
diabetes_train <- diabetes_data[trainIndex, ]
diabetes_train
#Assign the rows not selected in the trainIndex (30% of data) to the test dataset.
diabetes_test <- diabetes_data[-trainIndex, ]
```

# Logistic Regression Models

A logistic regression model is a generalized linear model that has a response that is a success/failure. In this case, 0 represents "failure" which is no diabetes and 1 represents success which is prediabetes. A logistic model uses a logit link (or log(odds)) to connect the average response with a linear function in the parameters.

### Logistic Model #1
Use all chosen predictors as main effect terms.
```{r}
set.seed(50)
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

log_fit_1 <- train(Diabetes_binary ~ HighBP + HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                   data = diabetes_train,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
log_fit_1
```

### Logistic Model #2
```{r}
set.seed(50)
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

log_fit_2 <- train(Diabetes_binary ~ HighBP*HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                   data = diabetes_train,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
log_fit_2
```

### Logistic Model #3
```{r}
set.seed(50)
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

log_fit_3 <- train(Diabetes_binary ~ HighBP + HighChol*HeartDiseaseorAttack + GenHlth*DiffWalk,
                   data = diabetes_train,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
log_fit_3
```




# Classification Tree

A classification tree is the idea of splitting up the predictor space into regions and having different predictions for each region. For a classification tree specifically, the goal is to predict group membership - no diabetes, or prediabetes/diabetes in this case. We will use the most prevalent class to predict. This method is very easy to understand, we don't need to include interaction terms, we don't need to scale, and we don't need to utilize statistical assumptions.

```{r}
#Set seed for reproducibility.
set.seed(50)
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

#Create the tuneGrid by making a dataframe of the cp parameter that starts at 0, goes to 0.1 and counts by 0.001.
tune_parameter <- data.frame(cp = seq(0, 0.1, by = 0.001))


#Create a classification tree using predictors of HighBP, HighChol, Heart Disease or Attack, General Health Rating, and Difficulty Walking to predict the presence of diabetes.
#Use rpart for the method.
#Use the train control defined above (5 fold cross validation) and the tuneGrid defined above.

diabetes_train

class_tree_diabetes <- train(Diabetes_binary ~ HighBP + HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                    data = diabetes_train,
                    method = "rpart",
                    metric = "logLoss",
                    trControl = trctrl,
                    tuneGrid = tune_parameter)
class_tree_diabetes
```
# Random Forest

A random forest extends the idea of bootstrap aggregation, but uses a random subset of predictors rather than all predictors. It can reduce the variance in comparison to a basic classification tree because you are averaging across trees. 

```{r}
#Use rf for the method.
#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid is from 1 to 5 because I have chosen five predictors.
#Set seed for reproducibility and do 3 repeats of 10 fold cross validation.
set.seed(50)
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)


rf_model_diabetes <- train(Diabetes_binary ~ HighBP + HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                    data = diabetes_train,
                    method = "rf",
                    metric = "logLoss",
                    trControl = trctrl,
                    tuneGrid = data.frame(mtry = 1:5))

rf_model_diabetes

```



# Final Model Selection