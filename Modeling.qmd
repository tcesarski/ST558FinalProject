---
title: "ST 558 Final Project Modeling Document"
format: html
author: "Taylor Cesarski"
---

# Introduction

The ultimate goal of modeling is to find the best predictive model (logistic regression, classification tree, and random forest) that can be used in the prediction of the diagnosis of diabetes. This is beneficial because diabetes is a very prevalent chronic disease. By using a predictive model, we could potentially provide earlier diagnosis and/or monitor patients that have a higher likelihood of developing diabetes due to certain risk factors. We want to be able to predict the likelihood of diabetes given someone has high blood pressure, high cholesterol, and other risk factors.

Based on the previous EDA, I have identified 5 significant predictors: high blood pressure, high cholesterol, heart disease or attack, general health rating, and difficulty walking. Information on the target variable and the predictor variables is below.

The Diabetes_binary column takes on 2 values: 0 for no diabetes, 1 for prediabetes or diabetes.

The HighBP column takes on 2 values: 0 for no high blood pressure and 1 for high blood pressure.

The HighChol column takes on 2 values: 0 for no high cholesterol and 1 for high cholesterol.

The HeartDiseaseorAttack column takes on 2 values: 0 if no coronary heart disease or myocardial infarction and 1 if have coronary heart disease or myocardial infarction.

The GenHlth column takes on 5 values based on rating of general health: 1 is excellent, 2 is very good, 3 good, 4 is fair, and 5 is poor.

The DiffWalk column takes on 2 values: 0 if no difficulty climbing stairs or walking and 1 if difficulty climbing stairs or walking.

# logLoss
logLoss is calculated using the following formula:
![Log Loss Formula](logloss.png)

When trying to classify, the model first predicts the probability of success. If the probability of success is greater than 0.5, it predicts success. If it is less than 0.5, it predicts failure. logLoss takes into account how far the predicted probability is to the true value (0 or 1). If there is a greater difference between the predicted value and the actual value, then the value of the logLoss will be higher. The logLoss value for each individual observation is calculated using the formula above (what is inside the summation). Then the average of the logLoss score is determined to get the overall logLoss score for the model. A lower logLoss score indicates a better fit. If the logLoss score is 0, it shows that the predicted probabilities match the true outcomes perfectly. The values of logLoss can range from 0 to infinity. The benefits of logLoss is that it not only accounts for accuracy, but also confidence in predictions. In classification, we can use the accuracy metric which is just correct predictions/total predictions. However, this does not give any information about any incorrect predictions being made. By using logLoss instead, we take into account both the accuracy and the confidence in predictions. 

# Read in data
In this section we will read in the data again and convert relevant variables that will be used in the model to factors.
```{r, message = FALSE, warning= FALSE}
library(tidyverse)
diabetes_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

diabetes_data$Diabetes_binary <- factor(diabetes_data$Diabetes_binary, levels = c(0, 1), labels = c("no_diabetes", "diabetes"))

diabetes_data$HighBP <- factor(diabetes_data$HighBP, levels = c(0, 1), labels = c("no_hbp", "hbp"))

diabetes_data$HighChol <- factor(diabetes_data$HighChol, levels = c(0, 1), labels = c("no_high_chol", "high_chol"))

diabetes_data$HeartDiseaseorAttack <- factor(diabetes_data$HeartDiseaseorAttack, levels = c(0, 1), labels = c("no_hd_or_attack", "hd_or_attack"))

diabetes_data$GenHlth <- factor(diabetes_data$GenHlth, levels = c(1, 2, 3, 4, 5), labels = c("excellent", "very_good", "good", "fair", "poor"))

diabetes_data$DiffWalk <- factor(diabetes_data$DiffWalk, levels = c(0, 1), labels = c("no_difficulty", "difficulty"))
```


# Split the Data
First we are going to split the data into a training and test set. We do this so that we can evaluate the model on data that wasn't used to fit the model. This helps us not to overfit the model and make sure that the model can generalize to data it hasn't yet seen.
```{r, message = FALSE, warning = FALSE}
library(caret)
#Set the seed at 100 for reproducibility.
set.seed(100)

#Use the createDataPartition from the caret package. Get 70% of the rows.
trainIndex <- createDataPartition(diabetes_data$Diabetes_binary, p =0.7, list = FALSE)
#Assign those 70% to the training dataset.
diabetes_train <- diabetes_data[trainIndex, ]
#Assign the rows not selected in the trainIndex (30% of data) to the test dataset.
diabetes_test <- diabetes_data[-trainIndex, ]
```

# Logistic Regression Models

A logistic regression model is a generalized linear model that has a response which is a success/failure. In this case, 0 represents "failure" which is no diabetes and 1 represents success which is diabetes or prediabetes. A logistic model uses a logit link (or log(odds)) to connect the average response with a linear function in the parameters.

### Logistic Model #1
Use all chosen predictors as main effect terms.
```{r}
#Set the seed for reproducibility.
set.seed(50)
#Use 5 fold cross validation. To use logLoss as a metric later, set classProbs = TRUE and summaryFunction = mnLogLoss.
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

#Train the first logistic model using all 5 main effect terms.
#Use the training data set.
#The method is glm (generalized linear model) with family as binomial to fit logistic.
#Preprocess the data by centering and scaling.
#Use the train control above to tell the model how to train.
log_fit_1 <- train(Diabetes_binary ~ HighBP + HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                   data = diabetes_train,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
#Print out information about the model. The logLoss was 0.3337.
log_fit_1
```

### Logistic Model #2
Use all chosen predictors as main effect terms and an interaction between High Blood Pressure and High Cholesterol.
```{r}
#Set the seed for reproducibility.
set.seed(50)
#Use 5 fold cross validation. To use logLoss as a metric later, set classProbs = TRUE and summaryFunction = mnLogLoss.
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)
#Train the second logistic model using all 5 main effect terms and an interaction between High Blood Pressure & High Cholesterol.
#Use the training data set.
#The method is glm (generalized linear model) with family as binomial to fit logistic.
#Preprocess the data by centering and scaling.
#Use the train control above to tell the model how to train.
log_fit_2 <- train(Diabetes_binary ~ HighBP*HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                   data = diabetes_train,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
#Print out information about the model. The logLoss was 0.3336.
log_fit_2
```

### Logistic Model #3
Use all chosen predictors as main effect terms, an interaction term between High Cholesterol and Heart Disease or Attack, and an interaction between General Health and Difficulty Walking.
```{r}
#Set the seed for reproducibility.
set.seed(50)
#Use 5 fold cross validation. To use logLoss as a metric later, set classProbs = TRUE and summaryFunction = mnLogLoss.
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

#Train the third logistic model using all 5 main effect terms and interaction terms between High Cholesterol and Heart Disease/Attack and an interaction between General Health and Difficulty Walking.
#Use the training data set.
#The method is glm (generalized linear model) with family as binomial to fit logistic.
#Preprocess the data by centering and scaling.
#Use the train control above to tell the model how to train.
log_fit_3 <- train(Diabetes_binary ~ HighBP + HighChol*HeartDiseaseorAttack + GenHlth*DiffWalk,
                   data = diabetes_train,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
#Print out information about the model. The logLoss was 0.3327.
log_fit_3
```

## Best Logistic Model 
Based on the three previous logistic models, Logistic Model #3 appears to be the best with the lowest logloss metric (0.3327) on the training set.


# Classification Tree

A classification tree is the idea of splitting up the predictor space into regions and having different predictions for each region. For a classification tree specifically, the goal is to predict group membership - no diabetes, or prediabetes/diabetes in this case. We will use the most prevalent class to predict. This method is very easy to understand, we don't need to include interaction terms, we don't need to scale, and we don't need to utilize statistical assumptions. There are some downsides to classification trees however including the use of a greedy algorithm that only looks one step ahead, a small change in the data drastically changing the tree, and they generally need to be pruned.

```{r}
#Set seed for reproducibility.
set.seed(50)
#Use 5 fold cross validation. To use logLoss as a metric later, set classProbs = TRUE and summaryFunction = mnLogLoss.
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)

#Create the tuneGrid by making a dataframe of the cp parameter that starts at 0, goes to 0.1 and counts by 0.001.
tune_parameter <- data.frame(cp = seq(0, 0.1, by = 0.001))


#Create a classification tree using predictors of HighBP, HighChol, Heart Disease or Attack, General Health Rating, and Difficulty Walking to predict the presence of diabetes.
#Use rpart for the method.
#Use the train control defined above (5 fold cross validation) and the tuneGrid defined above.
class_tree_diabetes <- train(Diabetes_binary ~ HighBP + HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                    data = diabetes_train,
                    method = "rpart",
                    metric = "logLoss",
                    trControl = trctrl,
                    tuneGrid = tune_parameter)
#Print out information about the model. The optimal model has a logLoss of 0.3602 with teh optimal complexity parameter being 0.
class_tree_diabetes
```
# Random Forest

A random forest extends the idea of bootstrap aggregation, but uses a random subset of predictors rather than all predictors. It can reduce the variance in comparison to a basic classification tree because you are averaging across trees. However, after running this model multiple times both with and without the use of the caret package, it appears that a random forest model may not be the best fit for this dataset using a logloss metric. This is because the class probabilities get driven *very* close to 0 or *very* close to 1 in comparison to other models. Therefore the logLoss is much higher than that of the other models. 
```{r}
#Set seed for reproducibility.
set.seed(50)
#Use 5 fold cross validation. To use logLoss as a metric later, set classProbs = TRUE and summaryFunction = mnLogLoss.
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = TRUE, 
                       summaryFunction = mnLogLoss)
#Create a random forest model using predictors determined by EDA.
#Use rf for the method.
#Use the train control defined above (5 fold cross validation) and the tuneGrid of mtry from 1 to 5 because there are five predictors.
rf_model_diabetes <- train(Diabetes_binary ~ HighBP + HighChol + HeartDiseaseorAttack + GenHlth + DiffWalk,
                    data = diabetes_train,
                    method = "rf",
                    metric = "logLoss",
                    trControl = trctrl, 
                    tuneGrid = data.frame(mtry = 1:5))
#Print out information about the model. The optimal tuning parameter was 
rf_model_diabetes
```

# Final Model Selection
We will compare all three models on the test set. Based on the test set, it appears that the logistic regression is the best model. On the test set, the logistic model has a logLoss of 0.3333, the classification tree has a logLoss of 0.3603, and the random forest model has a logLoss of infinity.

```{r, message = FALSE, warning=FALSE}
#Logistic Model
library(Metrics)
#Get actual values. Converts to 1 & 2 so subtract 1 to get 0 and 1.
actual <- as.numeric(diabetes_test$Diabetes_binary) - 1
#Use the best logistic model (log_fit_3) to predict the probability of success (diabetes).
predicted <- predict(log_fit_3, newdata = diabetes_test, type = "prob")
#Just get the predicted for diabetes (don't need no_diabetes).
predicted_diabetes <- predicted$diabetes
#Use logLoss function from Metrics package to compute logLoss.
logLoss(actual, predicted_diabetes)

#Classification Tree
#Get actual values. Converts to 1 & 2 so subtract 1 to get 0 and 1.
actual <- as.numeric(diabetes_test$Diabetes_binary) - 1
#Use the classification tree model (class_tree_diabetes) to predict the probability of success (diabetes). 
predicted <- predict(class_tree_diabetes, newdata = diabetes_test, type = "prob")
#Just get the predicted for diabetes (don't need no_diabetes).
predicted_diabetes <- predicted$diabetes
#Use logLoss function from Metrics package to compute logLoss.
logLoss(actual, predicted_diabetes)

#Random Forest
#Get actual values. Converts to 1 & 2 so subtract 1 to get 0 and 1.
actual <- as.numeric(diabetes_test$Diabetes_binary) - 1
#Use random forest model (rf_model_diabetes) to predict probability of success (diabetes). 
predicted <- predict(rf_model_diabetes, newdata = diabetes_test, type = "prob")
#Just get the predicted for diabetes (don't need no_diabetes).
predicted_diabetes <- predicted$diabetes
#Use logLoss function from Metrics package to computer logLoss.
logLoss(actual, predicted_diabetes)

```

